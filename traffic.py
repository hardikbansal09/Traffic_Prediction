# -*- coding: utf-8 -*-
"""Traffic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10evLWNaKLDUwapn7UfCJiG1DXIKHE79A
"""

from google.colab import files
uploaded=files.upload()

import pandas as pd
df=pd.read_csv('/content/Traffic.csv')
print(df)

df.isnull().sum()

import seaborn as sns
import matplotlib.pyplot as plt

# Plot boxplot for a single column
plt.figure(figsize=(10, 6))
sns.boxplot(x=df['Time'])  # Replace 'column_name' with the column you want to check
plt.title('Boxplot for Outliers')
plt.show()

df.dtypes

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import LabelEncoder

df['Time']=pd.to_datetime(df['Time'], format='%I:%M:%S %p')
df['Hour']=df['Time'].dt.hour

print(df)

# Create a 'Weekend' feature
print("Adding 'Weekend' feature...")
df['Is_Weekend'] = df['Day of the week'].isin(['Saturday', 'Sunday']).astype(int)

# Encode 'Traffic Situation' as numeric labels
print("Encoding 'Traffic Situation'...")
le = LabelEncoder()
df['Traffic Situation'] = le.fit_transform(df['Traffic Situation'])

# Drop unnecessary columns
print("Dropping unnecessary columns...")
df = df.drop(columns=['Time', 'Date', 'Day of the week'])

# Check dataset after preprocessing
print("Preprocessed Dataset:\n", df.head())

# ===============================
# Step 2: Exploratory Data Analysis
# ===============================
print("Performing Exploratory Data Analysis...")
plt.figure(figsize=(12, 6))
sns.countplot(x='Hour', data=df, palette='viridis')
plt.title('Traffic Count by Hour')
plt.xlabel('Hour of the Day')
plt.ylabel('Count')
plt.show()

# Correlation heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Heatmap')
plt.show()

# ============================
# Step 3: Model Development
# ============================
print("Developing Machine Learning Model...")
# Split data into features (X) and target (y)
X = df.drop(columns=['Traffic Situation'])
y = df['Traffic Situation']

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Random Forest Classifier
model = RandomForestClassifier(random_state=42, n_estimators=100)
model.fit(X_train, y_train)

# Predict on the test set
y_pred = model.predict(X_test)

# Evaluate model performance
print("Classification Report:\n", classification_report(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Accuracy Score:", accuracy_score(y_test, y_pred))

# ===============================
# Step 4: Visualizing Predictions
# ===============================
print("Visualizing Predictions...")
prediction_results = pd.DataFrame({
    'Actual': le.inverse_transform(y_test),
    'Predicted': le.inverse_transform(y_pred)
})
print("Sample Predictions:\n", prediction_results.head())

